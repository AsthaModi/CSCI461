{"cells":[{"cell_type":"markdown","metadata":{"id":"Nq79ToiM0ieV"},"source":["# Homework 1 - K Means Clustering\n","For HW1, you will apply k-means clustering to real environmental datasets. This assignment is designed to provide hands-on practice with a commonly used unsupervised learning algorithm.\n","\n","***MAKE YOUR OWN COPY OF THIS FILE BEFORE YOU START.***\n","\n","## Sections:\n","- **Random Initialization**: The first step of k-means requires selecting a number of clusters, K, and randomly selecting the centroid of each cluster.\n","- **Assign Points**: The next step is to assign a cluster to each data point in your data set. The cluster assignments should correspond to the cluster it is closet to when using euclidean distance.\n","- **Compute Centroids**: With the new cluster assignments, compute the new centroid of each cluster.\n","- **K-Means Clustering**: Repeat the steps from *Assign Points* and *Compute Centroids* until the centroids remain unchanged.\n","\n","- **Real-World Applications**\n","  - Climate\n","  - Wildfire (*MS Student Only*): Measure the quality of clusters using **silhouette scores**.\n","\n","\n","\n","## To-Do List\n","Look out for sections marked \"# IMPLEMENT\" and \"# QUESTION\"\n","- **Undergrads**: 4 Implement Blocks + 1 Question Block = 5 Points Total\n","- **Masters**: 6 Implement Blocks + 2 Question Block = 8 Points Total\n","\n","Complete each task and submit your Jupyter Notebook on Blackboard. Partial credit can be earned.\n"]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","\n"],"metadata":{"id":"mrvLVQjRL-Uc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7HK41FqatMul"},"outputs":[],"source":["# Import Useful Libraries\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math\n","np.random.seed(500)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nh_Oup5Q1mU9"},"outputs":[],"source":["# Randomly Generate Example Data\n","example_X = np.concatenate([np.random.multivariate_normal((3,3),((1,0),(0,1)), 30),\n","                           np.random.multivariate_normal((-3,3),((1,-0.5),(-0.5,1)), 30),\n","                           np.random.multivariate_normal((3,-3),((1,0),(0,1)), 30)])\n","\n","print(example_X.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iTP4YkYF2p9H"},"outputs":[],"source":["# Visualize Example Data\n","plt.scatter(example_X[:,0], example_X[:,1])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"y4b5PH309q3v"},"source":["## [1] Random Initialization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tT397YAAz4Eg"},"outputs":[],"source":["\"\"\"\n","\n","Randomly initialize centroids for k-means clustering based on points X.\n","X is passed in as a numpy array. You should randomly choose K different\n","points from X.\n","\n","Output should be a numpy array of size KxM where K is the number of centroids\n","and M is the number of features.\n","\n","\"\"\"\n","\n","def random_initialization(X: np.array, k: int):\n","    # -------------------------------------------------------------------------\n","    # IMPLEMENT - 1 Point\n","    # -------------------------------------------------------------------------\n","\n","    return initial_centroids"]},{"cell_type":"markdown","metadata":{"id":"8NkcHmYz9sqk"},"source":["## [2] Assign Points"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qHavlaUX3rwF"},"outputs":[],"source":["\"\"\"\n","\n","Given dataset X, \"cluster\" (assign) each point to the centroid that is closest\n","according to euclidean distance. You may implement euclidean distance yourself\n","or use sklearn's implementation.\n","\n","Output should be a numpy array of shape Nx1 where N is the number of data points.\n","\n","\"\"\"\n","\n","from sklearn.metrics.pairwise import euclidean_distances\n","\n","def assign_points(X: np.array, centroids: np.array):\n","    # -------------------------------------------------------------------------\n","    # IMPLEMENT - 1 Point\n","    # -------------------------------------------------------------------------\n","\n","    return point_assignments"]},{"cell_type":"markdown","metadata":{"id":"JDd8BhK19wYG"},"source":["## [3] Compute Centroids"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l2MxBIvS4lHu"},"outputs":[],"source":["\"\"\"\n","\n","Given dataset X (size NxM), and point assignments (Nx1), compute the new centroids of each cluster.\n","\n","Output should be a numpy array of shape (KxM), where K is the number of clusters, and M is the number of features.\n","\n","\"\"\"\n","\n","def compute_centroids(X: np.array, point_assignments: np.array, k: int):\n","    # -------------------------------------------------------------------------\n","    # IMPLEMENT - 1 Point\n","    # -------------------------------------------------------------------------\n","\n","    return centroids"]},{"cell_type":"markdown","metadata":{"id":"1yxNqt_t9x3W"},"source":["## [4] K-Means Clustering"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FQW9l3qe5Rot"},"outputs":[],"source":["\"\"\"\n","\n","Given dataset X and K clusters, cluster data points into K categories using K-Means clustering.\n","\n","To do this you need to:\n","    1. Initialize Cluster Centroids\n","    2. Assign Each Data Point to Closest Cluster Centroid\n","    3. Calculate New Cluster Centroid\n","Repeat steps 2 and 3 until the cluster centers remain unchanged.\n","\n","Utilize random_initalize(), assign_points(), an compute_centroids() in your implementation.\n","\n","This should return clusters as a numpy array (Nx1) with one entry for each point in X,\n","with entries corresponding to cluster ids from 0 to K-1.\n","\n","\n","\"\"\"\n","\n","def KMeans_cluster(X: np.array, k: int):\n","    # -------------------------------------------------------------------------\n","    # IMPLEMENT - 1 Point\n","    # -------------------------------------------------------------------------\n","\n","    return final_cluster_assignments"]},{"cell_type":"markdown","metadata":{"id":"VhrDqFYl6yhT"},"source":["Let's test our code on the example dataset and visualize the cluster assignments."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xkjv81yB6WQM"},"outputs":[],"source":["cluster_assignments = KMeans_cluster(X = example_X, k = 3)\n","\n","plt.scatter(example_X[:,0], example_X[:,1], c=cluster_assignments)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"GvT3_Xlc77US"},"source":["## [5] Real-World Application - Climate\n","In this real-world example, you will visualize global eco-regions. Eco-regions are areas where the ecosystem (and the type, quality, and quantity of environmental resources) are similar.\n","\n"," The provided raw data is geospatial data in the form of *'rasters'*. The rasters represent a grid overlay over a geographic area where each cell is associated with one or more values. In our case, each 10x10m grid cell across the world is associated with a set of environmental variables such as temperature and precipitation.\n","\n"," We have 3 datasets: one for the present environmental values of each grid cell, and two for projected climates in year 2050 under two different climate change scenarios (rcp2.6 and rcp8.5).\n","\n"," This is the well known WORLDCLIM dataset - https://www.worldclim.org/data/index.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Liq-Vz77-UR"},"outputs":[],"source":["# Gather Data\n","!pip install rasterio\n","!wget -Nq http://biogeo.ucdavis.edu/data/climate/worldclim/1_4/grid/cur/bio_10m_esri.zip    # Current Environmental Data\n","!wget -Nq http://biogeo.ucdavis.edu/data/climate/cmip5/10m/bc85bi50.zip                     # Projected 2050 Climate Scenario rcp8.5\n","!wget -Nq http://biogeo.ucdavis.edu/data/climate/cmip5/10m/bc26bi50.zip                     # Projected 2050 Climate Scenario rcp2.6\n","\n","# Unzip Datasources\n","!unzip -nq bio_10m_esri.zip\n","!unzip -nq bc85bi50.zip -d projection_data\n","!unzip -nq bc26bi50.zip -d projection_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WHL-SrdV8PB_"},"outputs":[],"source":["# Import Useful Libraries\n","%matplotlib inline\n","import os                           # cross platform manipulation of filesystem\n","import numpy as np                  # matrix library\n","import scipy\n","import scipy.spatial                # for distance calculations\n","import rasterio                     # read in and handle rasters\n","import matplotlib                   # plotting library\n","import matplotlib.pyplot as plt\n","import seaborn as sns               # plotting library expansion\n","from sklearn import preprocessing   # for preprocessing and storing data transformations\n","from sklearn import metrics         # for quantitatively evaluating our ml models\n","from scipy import stats             # statistics library for describing data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9tIbmN218Tbr"},"outputs":[],"source":["# Ordered List of Features/Data Points Available in Raw Data (can also be found here: https://www.worldclim.org/data/bioclim.html)\n","feature_names = [\n","  'Annual Mean Temperature',\n","  'Mean Diurnal Range (Mean of monthly (max temp - min temp))',\n","  'Isothermality (BIO2/BIO7) (* 100)',\n","  'Temperature Seasonality (standard deviation *100)',\n","  'Max Temperature of Warmest Month',\n","  'Min Temperature of Coldest Month',\n","  'Temperature Annual Range (BIO5-BIO6)',\n","  'Mean Temperature of Wettest Quarter',\n","  'Mean Temperature of Driest Quarter',\n","  'Mean Temperature of Warmest Quarter',\n","  'Mean Temperature of Coldest Quarter',\n","  'Annual Precipitation',\n","  'Precipitation of Wettest Month',\n","  'Precipitation of Driest Month',\n","  'Precipitation Seasonality (Coefficient of Variation)',\n","  'Precipitation of Wettest Quarter',\n","  'Precipitation of Driest Quarter',\n","  'Precipitation of Warmest Quarter',\n","  'Precipitation of Coldest Quarter'\n","]\n","\n","# Format Strings that Map to Desired Files. Using os.path.join allows us to create platform-independent paths.\n","current_feat_file_name_fmt = os.path.join(\"bio\",\"bio_{feat_ind}\",\"hdr.adf\")                 # Format for the current environmental data\n","future_feat_file_name_fmt = os.path.join(\"projection_data\",\"{model}{feat_ind}.tif\")         # Format for the projected data\n","\n","# Gather Current Feature Data\n","current_filenames = [current_feat_file_name_fmt.format(feat_ind=feat_ind) for feat_ind in range(1, len(feature_names)+1)]\n","\n","# Gather Future Feature Data for Each Model\n","    # Dictionary (hashtable) Mapping a model name to a list of feature names\n","    # We can look up the feature names for a given model name by running future_filenames[\"model\"]. Take a look at future_filenames.keys() to see what models there are.\n","model_names = [\"bc26bi50\",\"bc85bi50\"]\n","future_filenames = {model:[future_feat_file_name_fmt.format(model=model, feat_ind=feat_ind) for feat_ind in range(1, len(feature_names)+1)] for model in model_names}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4YPE0IDr8VZg"},"outputs":[],"source":["def load_features(file_names):\n","\n","    \"\"\"\n","\n","    Load masked rasters from a list of filenames.\n","    Loads all the data in the given input filename and mask away any square in the raster that does not have an observation for \"all\" of the features.\n","\n","    Args:\n","        file_names (str list) : List of filenames from which to read individual features\n","\n","    Returns:\n","        data (numpy masked array) : numpy matrix excluding entries with no data in any feature\n","\n","    \"\"\"\n","\n","    mask = None\n","    raw_data = []\n","    for file_name in file_names:\n","        # Open Raster for Reading the Raw Data\n","        with rasterio.open(file_name, \"r\") as f:\n","            data = f.read().squeeze().astype(np.float32)\n","        raw_data.append(data)\n","        if mask is None:\n","            mask = (data == f.nodata)\n","        else:\n","            mask = mask | (data == f.nodata)\n","\n","    # Mask Cells with No Data\n","    data = np.ma.array(np.stack(raw_data, axis=-1),mask=np.repeat(mask[:,:,None],len(file_names), axis=-1))\n","\n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FGqXI8HN8W9z"},"outputs":[],"source":["def show_map(data, title=None):\n","\n","    \"\"\"\n","\n","    Plots data for a scalar map.\n","\n","    Args:\n","        data (numpy array) : raster of values to plot\n","\n","    \"\"\"\n","\n","    plt.figure(figsize=(10,7))\n","    plt.imshow(data, cmap=\"Blues\")\n","    plt.colorbar(fraction=0.03, pad=0.04)\n","    plt.grid(\"off\")\n","    if title is not None:\n","        plt.title(title, fontsize=16)\n","    plt.show()\n","    plt.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NfdFFaXz8btp"},"outputs":[],"source":["# Load Feature Data for Current Data and Projections\n","current_features = load_features(current_filenames)\n","projected_features = {model: load_features(model_files) for model, model_files in future_filenames.items()}\n","\n","# Create empty raster of the same size and mask to plot useful things later\n","empty_raster = np.ma.array(np.zeros_like(current_features[:,:,0]),mask=current_features.mask[:,:,0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qC0OYXM98fRU"},"outputs":[],"source":["def flatten_mask(data):\n","    return data[~data.mask[:,:,0],:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"00h6iekM8nLO"},"outputs":[],"source":["current_observations = flatten_mask(current_features)\n","projected_observations = {model: flatten_mask(projected_features[model]) for model, model_files in future_filenames.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bEtjclsN8lDL"},"outputs":[],"source":["print(\"current indicators shape:\",current_observations.shape)\n","print(\"best case scenario shape:\",projected_observations[\"bc26bi50\"].shape)"]},{"cell_type":"markdown","source":["Now that the data has been loaded, we need to conduct some data pre-processing steps, namely transforming and rescaling the features.\n","\n","Below we are using a customized log transform, which is a common method for normalizing datasets that are heavily skewed. For example, you can see in the charts generated from the cell below that the distribution for annual precipitation is heavily skewed to the right while the histogram for average max temperature follows a more normal distribution.\n","\n","For this dataset, we will perform transformations on all features related to preciptation."],"metadata":{"id":"prGLiQZoDDbg"}},{"cell_type":"code","source":["# Annual Precipitation Distribution\n","plt.hist(current_observations[:,[11]],bins=50)\n","plt.title('Annual Precipitation')\n","plt.show()\n","\n","# Annual Mean Temperature Distribution\n","plt.hist(current_observations[:,[4]],bins=50)\n","plt.title('Mean Max Temperature')\n","plt.show()"],"metadata":{"id":"u1gfJBnbp3qi"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qp1BMtLm8o3m"},"outputs":[],"source":["# Define Transformations\n","def log_preprocess(data, log_features=None):\n","    res = data.copy()\n","    if log_features is not None:\n","        res[:,log_features] = np.log(res[:,log_features] + 1)\n","    return res"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ivYs-_-ba7Dh"},"outputs":[],"source":["def log_inv(data, log_features=None):\n","    res = data.copy()\n","    if log_features is not None:\n","        res[:,log_features] = np.exp(res[:log_features]) - 1\n","    return res"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MQackWWc8yoW"},"outputs":[],"source":["# Indices of Features Needing Transformation\n","log_features = [11,12,13,14,15,16,17,18]\n","\n","# Execute Transformations\n","log_transform = preprocessing.FunctionTransformer(func=log_preprocess,inverse_func=log_inv,kw_args={\"log_features\":log_features},check_inverse=False)"]},{"cell_type":"markdown","source":["Now that the data has been transformed, we will scale our data. Scaling data is an important step to ensure features with higher ranges of values do not dominate the algorithm."],"metadata":{"id":"nYOgPIgdsuU1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UhYqSDxv8zFR"},"outputs":[],"source":["# Define Scaler\n","scaler = preprocessing.StandardScaler(copy=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Jxc3yJh80sv"},"outputs":[],"source":["# Scale Data\n","processed_current_observations = scaler.fit_transform(log_transform.fit_transform(current_observations))\n","processed_projections = {model: scaler.transform(log_transform.fit_transform(model_observations)) for model, model_observations in projected_observations.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"52Bxpuq384si"},"outputs":[],"source":["# Plot Clusters\n","def plot_cluster(labels, title=None, empty_raster=empty_raster):\n","    label_raster = empty_raster.copy()\n","    label_raster[~label_raster.mask] = labels\n","    plt.figure(figsize=(18,6))\n","    im = plt.imshow(label_raster, cmap='tab20', vmin=0, vmax=labels.max())\n","    plt.colorbar(im, fraction=0.046, pad=0.04)\n","    if title is not None:\n","        plt.title(title, fontsize=16)\n","    plt.grid(\"off\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i2sJUqsa9ABI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692822695941,"user_tz":420,"elapsed":126,"user":{"displayName":"Hannah Murray","userId":"04756200053445396803"}},"outputId":"edc889fc-54dd-41d3-ee5e-57ab975284f3"},"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 8.53 ms, sys: 5.18 ms, total: 13.7 ms\n","Wall time: 14.3 ms\n"]}],"source":["%%time\n","training_data = np.concatenate([processed_current_observations, processed_projections[\"bc26bi50\"]])"]},{"cell_type":"markdown","source":["# Cluster Climate Data with K-Means\n","\n","*Note*: If you cannot get your own implementation of K-Means clustering to work, replace the call to your function in the code below and call the sklearn implementation of batched K-Means. Demonstrate the results for several number of clusters and check how regions change based on number of clusters (for partial credit)."],"metadata":{"id":"pQyKF_gGDSUp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4OvlgdJA9CU3"},"outputs":[],"source":["current_labels = KMeans_cluster(processed_current_observations, k=3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SrUYIBfQ9EY5"},"outputs":[],"source":["plot_cluster(current_labels, \"Clustering of Current Data\")"]},{"cell_type":"code","source":["'''\n","\n","Q: Are Canada and Europe in the same cluster?\n","\n","'''\n","\n","# -------------------------------------------------------------------------\n","# QUESTION - 1 Point\n","# -------------------------------------------------------------------------\n","\n","# Your answer."],"metadata":{"id":"dvURhygJu-s_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Optional Exploration**: Using the code below, re-run the clustering with the same K (but different initializations because of the randomness). What happens to the results? Next, adjust the code to run the clustering with a larger K=4,6,8. Comment on what you observe with the resulting maps."],"metadata":{"id":"IYJvesczDcCw"}},{"cell_type":"code","source":["k=4\n","current_labels = KMeans_cluster(processed_current_observations, k)\n","plot_cluster(current_labels, \"Clustering of Current Data\")"],"metadata":{"id":"zqttpr9lDjSs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["k=6\n","current_labels = KMeans_cluster(processed_current_observations, k)\n","plot_cluster(current_labels, \"Clustering of Current Data\")"],"metadata":{"id":"Gv8PnskRbl6q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["k=8\n","current_labels = KMeans_cluster(processed_current_observations, k)\n","plot_cluster(current_labels, \"Clustering of Current Data\")"],"metadata":{"id":"fID7zqZHbnow"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","\n","OPTIONAL Q: What is your recommended K based on your manual inspection? Why?\n","\n","'''\n","\n","# -------------------------------------------------------------------------\n","# OPTIONAL QUESTION - 0 Points\n","# -------------------------------------------------------------------------\n","\n","# Your answer."],"metadata":{"id":"g9UZr8Nrdqoz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##  [6] Real-World Application - Wildfire *(MS Student Only)*"],"metadata":{"id":"Cd1os4B2psej"}},{"cell_type":"markdown","source":["The Fire Program Analysis Fire-Occurrence Database (FPA FOD) includes 1.88 million geo-referenced wildfire records from 1992 to 2015 (https://www.kaggle.com/datasets/rtatman/188-million-us-wildfires).\n","\n","The referenced `wildfire.csv` file extracts the wildfire size class from FPA FOD and filters for wildfire years 2001 - 2015 for all 50 states.\n","\n","In this section, you will\n","\n","1.   Extract the largest wildfire class for each year in each state.\n","2.   Map wildfire size class from letters to numbers (ex. Class A => 0, Class B => 1, so on...)."],"metadata":{"id":"lzBERLsSuqg0"}},{"cell_type":"markdown","source":["##### **Notes on Data**\n","`FIRE_SIZE_CLASS` represents fire size based on the number of acres within the final fire perimeter expenditures. The mappings are as follows:\n","\n","*   A = greater than 0 but less than or equal to 0.25 acres\n","*   B = 0.26-9.9 acres\n","*   C = 10.0-99.9 acres\n","*   D = 100-299 acres\n","*   E = 300 to 999 acres\n","*   F = 1000 to 4999 acres\n","*   G = 5000+ acres\n","\n","`wildfire.csv` maps the wildfire size class from letters to numbers (ex. Class A => 0, Class B => 1, etc.)."],"metadata":{"id":"B-z1aBr1uWbc"}},{"cell_type":"code","source":["# Gather Data\n","!wget -Nq https://raw.githubusercontent.com/csci461/dataset/main/wildfire.csv"],"metadata":{"id":"IEsN4zlVpqqo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import Useful Libraries\n","import plotly.express as px\n","import pandas as pd\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_score"],"metadata":{"id":"FFMESn0QqDYZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualize Tabular Data\n","df = pd.read_csv(\"wildfire.csv\",index_col=0)\n","df.head(10)        # Visualize first 10 rows"],"metadata":{"id":"l0pMRxHmsLdy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","\n","Use sklearn's built in K-Means clustering algorithm (set random state to 123, use default hyperparameters) and silhouette_score\n","\n","Cluster 50 states by wildfire size class from 2001 to 2015\n","\n","Use silhouette score to determine top 2 number of clusters (M clusters, N clusters)\n","\n","Show line plot - number of clusters (from 2 to 19) vs. silhouette score\n","\n","'''\n","\n","# -------------------------------------------------------------------------\n","# IMPLEMENT - 1 Point\n","# -------------------------------------------------------------------------\n"],"metadata":{"id":"4Tl204wNSeJi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cluster_map(df,cluster_label,title):\n","\n","    \"\"\"\n","\n","    Plot cluster labels on 50 states map\n","\n","    Args:\n","        df (pandas dataframe) : dataframe with states as index and year as columns\n","        cluster_label (numpy array) : cluster labels for 50 states\n","        title (str) : title for the plot\n","\n","    Returns:\n","        data (numpy masked array) : numpy matrix excluding entries with no data in any feature\n","\n","    \"\"\"\n","\n","    df_cluster = df.copy()\n","    df_cluster[\"Cluster\"] = [str(item) for item in list(cluster_label)]\n","    fig = px.choropleth(df_cluster,locations = df_cluster.index, locationmode=\"USA-states\", scope=\"usa\",\n","                      color=\"Cluster\",category_orders={\"Cluster\": np.sort(df_cluster[\"Cluster\"].unique())},\n","                      title=title)\n","    fig.show()"],"metadata":{"id":"0wv3X3kjrR-g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","\n","Cluster 50 states by wildfire size class from 2001 to 2015 with the best number of clusters M\n","\n","Show cluster map\n","\n","Cluster 50 states by wildfire size class from 2001 to 2015 with the 2nd best number of clusters N\n","\n","Show cluster map\n","\n","'''\n","\n","# -------------------------------------------------------------------------\n","# IMPLEMENT - 1 Point\n","# -------------------------------------------------------------------------\n","\n","\n","cluster_map(df,kmeans_labels_for_cluster_M,\"2000 - 2015 Largest Wildfire Size Class\")\n","cluster_map(df,kmeans_labels_for_cluster_N,\"2000 - 2015 Largest Wildfire Size Class\")"],"metadata":{"id":"Uwei4rVltBkz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import Image\n","Image(url=\"https://hazards.fema.gov/nri/Content/Images/StaticPageImages/map-wildfire_risk.png\", width=600, height=300)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":322},"id":"wIxj6Tiutgp7","executionInfo":{"status":"ok","timestamp":1661661246541,"user_tz":420,"elapsed":288,"user":{"displayName":"Hsien-Te Kao","userId":"05865654724135398937"}},"outputId":"44515fe7-327e-47ce-917b-c1560222ee9c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<img src=\"https://hazards.fema.gov/nri/Content/Images/StaticPageImages/map-wildfire_risk.png\" width=\"600\" height=\"300\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["'''\n","\n","Q: FEMA provides a wildfire risk map (see above). Look back at your cluster maps.\n","   Which one is the best match (visually) to the FEMA map? How many clusters?\n","\n","'''\n","\n","# -------------------------------------------------------------------------\n","# QUESTION - 1 Point\n","# -------------------------------------------------------------------------\n","\n","# Your answer."],"metadata":{"id":"FD3Rg01BTake"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1hBYN-7x7GmA5gP3WFsnnoQUI1PG_9gMd","timestamp":1661659284499},{"file_id":"19V8omDcN_8fRPizJYhSNu6qlFB4OyYC8","timestamp":1568268462363}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"nbformat":4,"nbformat_minor":0}